{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#BorutaSHAP\" data-toc-modified-id=\"BorutaSHAP-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>BorutaSHAP</a></span></li><li><span><a href=\"#Suggested-Example-Workflow\" data-toc-modified-id=\"Suggested-Example-Workflow-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Suggested Example Workflow</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Deal-with-potential-correlation-/-multicollinearity-first.\" data-toc-modified-id=\"Step-1:-Deal-with-potential-correlation-/-multicollinearity-first.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Step 1: Deal with potential correlation / multicollinearity first.</a></span></li><li><span><a href=\"#Step-2:-Run-BorutaSHAP\" data-toc-modified-id=\"Step-2:-Run-BorutaSHAP-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Step 2: Run BorutaSHAP</a></span></li><li><span><a href=\"#Step-3:-Fit-final-model-with-these-features\" data-toc-modified-id=\"Step-3:-Fit-final-model-with-these-features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Step 3: Fit final model with these features</a></span></li><li><span><a href=\"#Step-4:-Explain-using-SHAP-(or-another-method)\" data-toc-modified-id=\"Step-4:-Explain-using-SHAP-(or-another-method)-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Step 4: Explain using SHAP (or another method)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BorutaShap import BorutaShap\n",
    "from BorutaShap import load_data\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ml_inspector # From https://gitlab.nist.gov/gitlab/nam4/ml_inspector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp\n",
    "import hpsklearn\n",
    "from hpsklearn import HyperoptEstimator\n",
    "from hyperopt import tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -t -m -h -v --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BorutaSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = BorutaShap(model=None, # Defaults to sklearn RandomForest() and all its defaults - no RNG seed so not reproducible\n",
    "                      importance_measure='shap', # Can be 'gini' or 'shap'\n",
    "                      classification=False, # False for regression problems\n",
    "                      percentile=100, pvalue=0.05 # These are default values to behave like the original R package\n",
    "                     )\n",
    "\n",
    "# If 'shap': uses shap.TreeExplainer(self.model, feature_perturbation = \"tree_path_dependent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BorutaSHAP comes pre-loaded with some data as an example\n",
    "X, y = load_data(data_type='regression')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the feature selector\n",
    "# Train vs. test is an open question: https://compstat-lmu.github.io/iml_methods_limitations/pfi-data.html#introduction-to-test-vs.training-data\n",
    "selector.fit(X=X,  # Must be a Pandas DataFrame\n",
    "             y=y, \n",
    "             n_trials=100, \n",
    "             sample=False, # Use KS-test to choose a sample of the data to compute importance values on instead of entire dataset (for speed)\n",
    "             train_or_test='test', # Do internal 70/30 train/test split\n",
    "             normalize=True, # Normalize metric (i.e., shap value importance) by (x-x_mean)/x_std\n",
    "             verbose=True,\n",
    "             random_state=0 # For reproducibility, but only relevant if model in selector is also specified\n",
    "            )\n",
    "\n",
    "# If train_or_test == 'train' model is trained and evaluated on same data (entirely of X provided); however,\n",
    "# if 'test', a train_test_split with a 70:30 of X using the random_state value when fit() is called (see below).\n",
    "\n",
    "# The Bonferroni correction for multiple comparisons is used, as in the original R version of Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.plot(which_features='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(selector.columns, selector.shap_values), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.Subset().head() # Select just the columns that passed --> this is the data you can then train on\n",
    "\n",
    "# By default, all features that are weakly supported (tentative) are also included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.accepted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested Example Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In production, the following workflow is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# for y, 0 = Malignant, 1 = Benign \n",
    "\n",
    "# See description at: http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Deal with potential correlation / multicollinearity first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at multicollinearity\n",
    "selected_features, cluster_id_to_feature_ids = ml_inspector.data.InspectData.cluster_collinear(X, # Can use entire dataset since this is unsupervised\n",
    "                                                                              figsize=(12, 8), \n",
    "                                                                              display=True, \n",
    "                                                                              t=2,\n",
    "                                                                              feature_names=None) # None returns indices, otherwise can specify: data.feature_names.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Chose {} out of {} features'.format(len(selected_features), X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=0, objective='binary:logistic', eval_metric='logloss', seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This replicates the inner workings of BorutaSHAP\n",
    "X_train_, X_test_, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=0)\n",
    "\n",
    "# See what happens when all features are used\n",
    "clf.fit(X_train_, y_train)\n",
    "clf.score(X_test_, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit again just using only decorrelated subset of features\n",
    "X_train, X_test = X_train_[:,selected_features], X_test_[:,selected_features]\n",
    "clf.fit(X_train, y_train) \n",
    "clf.score(X_test, y_test) # Almost identical to using all features but greatly reduced (8/30)\n",
    "\n",
    "# Note that we could also be a little more lenient to allow more features to get through, with the possibility\n",
    "# that they are more correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run BorutaSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = BorutaShap(model=clf.__class__(**clf.get_params()),\n",
    "                      importance_measure='shap',\n",
    "                      classification=True, \n",
    "                      percentile=100, pvalue=0.05\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BorutaSHAP to see which of the decorrelated dimensions carry information\n",
    "X_decor = pd.DataFrame(data=X[:,selected_features], columns=data.feature_names[selected_features])\n",
    "\n",
    "selector.fit(X=X_decor, \n",
    "             y=y, \n",
    "             n_trials=100, \n",
    "             sample=False,\n",
    "             train_or_test='test', # Does internal 70:30 train/test split\n",
    "             normalize=True,\n",
    "             verbose=True,\n",
    "             random_state=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.plot(which_features='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.Subset().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selector.Subset().values, y, test_size=0.7, random_state=0)\n",
    "clf.fit(X_train, y_train) \n",
    "clf.score(X_test, y_test) \n",
    "\n",
    "# All features (93.23%, 30 features) > \n",
    "# decorrelated (92.98%, 8 features) > \n",
    "# BorutaSHAP(decorrelated) (92.73%, 6 features)\n",
    "# Overall performance is nearly identical\n",
    "\n",
    "# However - note that this is dependent upon the model chosen.  sklearn's classifiers, random forest, etc.\n",
    "# will give different results from these XGBClassifier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, see what happens when we don't initially decorrelate, and use all the features\n",
    "selector_no_decor = BorutaShap(model=clf.__class__(**clf.get_params()), \n",
    "                               importance_measure='shap',\n",
    "                               classification=True, \n",
    "                               percentile=100, pvalue=0.05\n",
    "                              )\n",
    "\n",
    "selector_no_decor.fit(X=pd.DataFrame(data=X, columns=data.feature_names),\n",
    "                      y=y, \n",
    "                      n_trials=100, \n",
    "                      sample=False,\n",
    "                      train_or_test='test', \n",
    "                      normalize=True,\n",
    "                      verbose=True,\n",
    "                      random_state=0\n",
    "                     )\n",
    "\n",
    "selector_no_decor.plot(which_features='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the fact that when no decorrelation performed, many high ranking features are actually considered correlated\n",
    "# For example, 'worst concave points' and 'mean concave points' are the 2 leading features here, which are\n",
    "# \"covered\" by 'mean compactness' in the decorrelated set; similarly, ('worst perimeter', 'worst area') --> 'mean radius'\n",
    "for k,v in sorted(cluster_id_to_feature_ids.items(), key=lambda x: x[0]):\n",
    "    print(k, [data.feature_names[i].upper() if i in selected_features else data.feature_names[i].lower() for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selector_no_decor.Subset().values, y, test_size=0.7, random_state=0)\n",
    "clf.fit(X_train, y_train) \n",
    "clf.score(X_test, y_test) \n",
    "\n",
    "# Slightly improved relative to using all features!\n",
    "# This sort of behavior has been noted before (https://medium.com/analytics-vidhya/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677)\n",
    "# The performance is essentially identical, and now it relies on features that are clearly correlated\n",
    "# and therefore expecting a reasonable explanation for these results will be difficult, e.g.,\n",
    "# perimeters and areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fit final model with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(selector.Subset().values, \n",
    "                                                    y, \n",
    "                                                    test_size=0.7, # Can be different from BorutaSHAP if desired \n",
    "                                                    random_state=0)\n",
    "clf.fit(X_train, y_train) \n",
    "print('{}% test accuracy for {} with hyperparameters: {}'.format('%.2f'%(100.0*clf.score(X_test, y_test)), \n",
    "                                                                 clf.__class__,\n",
    "                                                                 clf.get_params()\n",
    "                                                                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In reality, we would actually perform some optimization perhaps with auto-sklearn or hyperopt-sklearn\n",
    "# to optimize the final model. These pipelines may include regularization to economize the list of features\n",
    "# from BorutaSHAP, which only test if they carry information that can be used to make a prediction.\n",
    "\n",
    "# The following example uses hyperopt-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see help(hpsklearn.components._xgboost_hp_space)\n",
    "kwargs={'n_jobs':-1,\n",
    "        'random_state':0, # For reproducibility\n",
    "        'n_estimators':hp.uniformint('n_estimators', 1, 100),\n",
    "        'learning_rate':hp.loguniform('learning_rate', low=np.log(1e-5), high=np.log(1e-2)),\n",
    "        'max_depth':hp.uniformint('max_depth', 3, 10),\n",
    "       }\n",
    "\n",
    "# Instantiate a HyperoptEstimator with the search space and number of evaluations\n",
    "opt_estim = HyperoptEstimator(classifier=hpsklearn.components.xgboost_classification('XGBClassifier', **kwargs), \n",
    "                          preprocessing=[], # Trees don't need any scaling -NOTE: \"none\" = random choice not \"don't use any\"\n",
    "                          algo=tpe.suggest, # Use tree parzen estimator\n",
    "                          max_evals=100, # Total number of models to evaluate - USE MORE IN PRACTICE\n",
    "                          trial_timeout=10, # Allow this many seconds to fit each - USE LONGER IN PRACTICE\n",
    "                          seed=0, # For reproducibility\n",
    "                          refit=True) # Refit the best model on entire training set when done\n",
    "\n",
    "# Search the hyperparameter space based on the data\n",
    "opt_estim.fit(X=X_train, y=y_train, \n",
    "              valid_size=0.0, # Don't need separate validation set, use all of training\n",
    "              n_folds=3,  # Stratified by default - CAN USE MORE IN PRACTICE\n",
    "              cv_shuffle=True,\n",
    "              random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These results might seem lower than the simple, first pass - however, this is a cross-validated result.\n",
    "\n",
    "# This is automatically done by hyperopt-sklearn, but just make sure the model is retrained on all of the \n",
    "# training data, not just k-1 folds.\n",
    "model = opt_estim.best_model()['learner']\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('{}% test accuracy, {}% train accuracy'.format('%.2f'%(100.0*opt_estim.score(X_test, y_test)), \n",
    "                                                     '%.2f'%(100.0*opt_estim.score(X_train, y_train))\n",
    "                                                    )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explain using SHAP (or another method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selector above was fit with shadow features so the result will not be identical to this.\n",
    "# Here, we perform the SHAP analysis again but now with the final model.\n",
    "# Note: we can use more stringent samping parameters, etc. here since we don't have to iterate as we\n",
    "# have to in BorutaSHAP; this means we can be more accurate about the SHAP values at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the TreeExplainer since we have a tree-based model, or the more general KernelExplainer.\n",
    "# Both produce very similar results using the settings below.\n",
    "\n",
    "# Note: had we predicted log-odds for this binary classification task we would use 'logit' link functions\n",
    "# in the below examples and its 'raw' output.  Instead, we will analyze the output of predict_proba()\n",
    "\n",
    "use_tree = True\n",
    "\n",
    "if use_tree:\n",
    "    # if model output = raw it seems to just expaine the last column of the model prediction (class prob here)\n",
    "    explainer = shap.TreeExplainer(model=model,\n",
    "                                   model_output='predict_proba',\n",
    "                                   data=X_train\n",
    "                                  )\n",
    "    shap_values = explainer.shap_values(X_test, check_additivity=True) # The tree explainer can be checked for additivity (shap + prediction = mean)\n",
    "    # It seems that trying to do this explicitly later is problematic, apparently because of the way TreeExplainer works.\n",
    "else:\n",
    "    explainer = shap.KernelExplainer(model.predict_proba,\n",
    "                                     data=X_train, \n",
    "                                     link='identity') \n",
    "    shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "    # Additivity can be explicitly checked later when using the KernelExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these probabilities are clustered near 50% in this model so we are not explaining a large number - this is ok\n",
    "for i in [0,1]:\n",
    "    probs = model.predict_proba(X_test)[model.predict(X_test) == i][:,i]\n",
    "    mean, std= np.mean(probs), np.std(probs)\n",
    "    print('When i={}, mean(Prob)={} +/- {}'.format(i, mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It turns out that classes \"0\" and \"1\" are \"in order\" but this is not guaranteed\n",
    "print('Model stored classes as: {}'.format(model.classes_))\n",
    "\n",
    "# Probability is predicted for each class - since binary we can just look at one - CHECK WHAT THIS MEANS\n",
    "examine_class_idx = 0 \n",
    "\n",
    "print('Looking at class {} which means {}'.format(model.classes_[examine_class_idx],\n",
    "                                                  'malignant' if model.classes_[examine_class_idx] == 0 else 'benign'\n",
    "                                                 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall summary of average effect of each feature\n",
    "shap.summary_plot(shap_values, #[examine_class_idx], \n",
    "                  X_test, \n",
    "                  plot_type=\"bar\",\n",
    "                  feature_names=selector.Subset().columns\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A jittered violin plot is more helpful to summarize trends in the sign of SHAP value vs. prediction\n",
    "shap.summary_plot(shap_values[examine_class_idx], \n",
    "                  X_test, \n",
    "                  feature_names=selector.Subset().columns\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at decision paths\n",
    "misclassified = y_test != model.predict(X_test)\n",
    "\n",
    "shap.decision_plot(explainer.expected_value[examine_class_idx], \n",
    "                   shap_values[examine_class_idx], \n",
    "                   feature_names=selector.Subset().columns.tolist(),\n",
    "                   link='identity',\n",
    "                   feature_order='importance',\n",
    "                   highlight=misclassified\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the misclassified ones\n",
    "shap.decision_plot(explainer.expected_value[examine_class_idx], \n",
    "                   shap_values[examine_class_idx][misclassified,:], \n",
    "                   feature_names=selector.Subset().columns.tolist(),\n",
    "                   link='identity',\n",
    "                   feature_order='importance',\n",
    "                   title='ONLY THE MISCLASSIFIED DATA POINTS',\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See here for visualizing multioutput decision plots, which are useful when we have more than 2 classes:\n",
    "# https://slundberg.github.io/shap/notebooks/plots/decision_plot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[examine_class_idx], \n",
    "                shap_values[examine_class_idx], \n",
    "                X_test, \n",
    "                link='identity',\n",
    "                feature_names=selector.Subset().columns\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine an individual instance\n",
    "instance_idx = 123\n",
    "\n",
    "print('Correct answer = {}, predicted probabilities = {}'.format(y_test[instance_idx], \n",
    "                                                   model.predict_proba(X_test[instance_idx,:].reshape(1,-1))\n",
    "                                                  ))\n",
    "if not use_tree:\n",
    "    print('\\nThe SHAP values for this instance are:')\n",
    "    for i in range(shap_values[examine_class_idx].shape[1]):\n",
    "        print('{} = {}'.format(selector.Subset().columns[i], shap_values[examine_class_idx][instance_idx,i]))\n",
    "\n",
    "    print('\\nThese sum to {}'.format(np.sum(shap_values[examine_class_idx][instance_idx,:])))\n",
    "    pred_ = model.predict_proba(X_test[instance_idx,:].reshape(1,-1))[0][examine_class_idx]\n",
    "    # mean_ = model.predict_proba(X_train)[:,examine_class_idx])\n",
    "    mean_ = explainer.expected_value[examine_class_idx]\n",
    "    print('\\nIndeed, this should be the prediction - mean (from training set) = {} - {} = {}'.format(pred_,\n",
    "                                                                                                   mean_,\n",
    "                                                                                                   pred_ -mean_\n",
    "                                                                                                  ))\n",
    "    assert(np.abs((pred_ - mean_) - np.sum(shap_values[examine_class_idx][instance_idx,:])) < 1.0e-8)\n",
    "\n",
    "shap.force_plot(explainer.expected_value[examine_class_idx], \n",
    "                shap_values[examine_class_idx][instance_idx,:], \n",
    "                X_test[instance_idx,:], \n",
    "                link='identity',\n",
    "                feature_names=selector.Subset().columns\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical dispersion at a single value represents interaction effects with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean radius itself increases the probability of this class index if > 16 and decreases it if < 16\n",
    "shap.dependence_plot('mean radius', \n",
    "                     shap_values=shap_values[examine_class_idx], \n",
    "                     features=pd.DataFrame(data=X_test, columns=selector.Subset().columns)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean texture itself increases the probability of this class index if > 20 and decreases it if < 16\n",
    "\n",
    "# Overall the mean texture has less of an impact if the mean compactness is also low.\n",
    "shap.dependence_plot('mean texture', \n",
    "                     shap_values=shap_values[examine_class_idx], \n",
    "                     features=pd.DataFrame(data=X_test, columns=selector.Subset().columns)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean compactness itself increases the probability of this class index if > .12 and decreases it if < .12,\n",
    "# though there appear to be essentially 3 different \"levels\"\n",
    "\n",
    "# Overall the mean compactness (perimeter^2 / area - 1.0) has less of an impact if the radius error is high\n",
    "shap.dependence_plot('mean compactness', \n",
    "                     shap_values=shap_values[examine_class_idx], \n",
    "                     features=pd.DataFrame(data=X_test, columns=selector.Subset().columns)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean smoothness itself increases the probability of this class index if > .09 and decreases it if < .09,\n",
    "# though there appear to be essentially 3 different \"levels\"\n",
    "shap.dependence_plot('mean smoothness', \n",
    "                     shap_values=shap_values[examine_class_idx], \n",
    "                     features=pd.DataFrame(data=X_test, columns=selector.Subset().columns)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very little effect of texture error\n",
    "shap.dependence_plot('texture error', \n",
    "                     shap_values=shap_values[examine_class_idx], \n",
    "                     features=pd.DataFrame(data=X_test, columns=selector.Subset().columns)\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
